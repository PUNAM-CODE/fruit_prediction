In [ ]:
pip install googledrivedownloader
Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (0.4)
In [ ]:
from google_drive_downloader import GoogleDriveDownloader as gdd

gdd.download_file_from_google_drive(file_id='101lHWlBKAsXjPkVBTz1_Ge9S2rwVPTUU',
                                    dest_path='content/fruits_data.zip',
                                    unzip=True)
In [ ]:
import pandas as pd                                     # Data analysis and manipultion tool
import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays
import tensorflow as tf                                 # Deep Learning Tool
import os                                               # OS module in Python provides a way of using operating system dependent functionality
import cv2                                              # Library for image processing
from sklearn.model_selection import train_test_split    # For splitting the data into train and validation set
In [ ]:
labels = pd.read_csv("/content/content/fruits_data/Training_set.csv")   # loading the labels
labels.head()           # will display the first five rows in labels dataframe
Out[ ]:
filename	label
0	Image_1.jpg	Pear 2
1	Image_2.jpg	Tomato Heart
2	Image_3.jpg	Plum 3
3	Image_4.jpg	Pear Stone
4	Image_5.jpg	Cherry 2
In [ ]:
file_paths = [[fname, '/content/content/fruits_data/train/' + fname] for fname in labels['filename']]
In [ ]:
# Confirm if number of images is same as number of labels given
if len(labels) == len(file_paths):
    print('Number of labels i.e. ', len(labels), 'matches the number of filenames i.e. ', len(file_paths))
else:
    print('Number of labels does not match the number of filenames')
Number of labels i.e.  47384 matches the number of filenames i.e.  47384
In [ ]:
images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])
images.head()
Out[ ]:
filename	filepaths
0	Image_1.jpg	/content/content/fruits_data/train/Image_1.jpg
1	Image_2.jpg	/content/content/fruits_data/train/Image_2.jpg
2	Image_3.jpg	/content/content/fruits_data/train/Image_3.jpg
3	Image_4.jpg	/content/content/fruits_data/train/Image_4.jpg
4	Image_5.jpg	/content/content/fruits_data/train/Image_5.jpg
In [ ]:
train_data = pd.merge(images, labels, how = 'inner', on = 'filename')
train_data.head()
Out[ ]:
filename	filepaths	label
0	Image_1.jpg	/content/content/fruits_data/train/Image_1.jpg	Pear 2
1	Image_2.jpg	/content/content/fruits_data/train/Image_2.jpg	Tomato Heart
2	Image_3.jpg	/content/content/fruits_data/train/Image_3.jpg	Plum 3
3	Image_4.jpg	/content/content/fruits_data/train/Image_4.jpg	Pear Stone
4	Image_5.jpg	/content/content/fruits_data/train/Image_5.jpg	Cherry 2
In [ ]:
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train_data['label'] = le.fit_transform(train_data['label'])
In [ ]:
data = []     # initialize an empty numpy array
image_size = 100      # image size taken is 100 here. one can take other size too
for i in range(len(train_data)):
  
  img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_COLOR)   # converting the image to gray scale

  new_img_array = cv2.resize(img_array, (image_size, image_size))      # resizing the image array
  data.append([new_img_array, train_data['label'][i]])
In [ ]:
np.random.shuffle(data)
In [ ]:
x = []
y = []
for image in data:
  x.append(image[0])
  y.append(image[1])

# converting x & y to numpy array as they are list
x = np.array(x)
y = np.array(y)
In [ ]:
np.unique(y, return_counts=True)
Out[ ]:
(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,
         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,
         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,
         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,
         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,
         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,
         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,
         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,
        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,
        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,
        130]),
 array([344, 311, 336, 345, 337, 344, 319, 345, 344, 300, 343, 344, 470,
        345, 299, 344, 343, 315, 343, 315, 324, 343, 344, 344, 343, 491,
        344, 517, 517, 344, 344, 344, 315, 343, 343, 315, 324, 275, 328,
        343, 328, 491, 208, 343, 689, 344, 343, 343, 344, 330, 343, 344,
        343, 325, 343, 343, 326, 330, 343, 344, 343, 343, 343, 343, 343,
        298, 210, 343, 517, 344, 344, 336, 458, 374, 315, 312, 307, 335,
        345, 343, 344, 517, 344, 345, 487, 343, 491, 210, 343, 466, 498,
        343, 343, 311, 491, 466, 466, 344, 344, 343, 345, 343, 313, 294,
        630, 344, 315, 315, 317, 315, 315, 343, 344, 343, 345, 343, 344,
        517, 343, 343, 517, 470, 517, 335, 344, 479, 257, 321, 332, 515,
        333]))
In [ ]:
x =  x.reshape(-1, 100, 100, 3)
In [ ]:
# split the data
X_train, X_val, y_train, y_val = train_test_split(x,y,test_size=0.3, random_state = 42)
TPU time

In [ ]:
import math, re, os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Flatten, Reshape
print("Tensorflow version " + tf.__version__)
Tensorflow version 2.5.0
In [ ]:
# Detect TPU, return appropriate distribution strategy
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() 
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    strategy = tf.distribute.get_strategy() 

print("REPLICAS: ", strategy.num_replicas_in_sync)
REPLICAS:  1
In [ ]:
with strategy.scope():
    num_classes = 131
    i = tf.keras.layers.Input([100, 100, 3], dtype = tf.uint8)
    x = tf.cast(i, tf.float32)
    #x = tf.keras.applications.vgg16.preprocess_input(x)
    x =  tf.keras.applications.mobilenet_v2.preprocess_input(x)
    # pretrained_model = tf.keras.applications.vgg16.VGG16(
    #       weights='imagenet',
    #       include_top=False ,
    #       input_shape=[100, 100, 3]
    # )
    pretrained_model = tf.keras.applications.mobilenet_v2.MobileNetV2(
          weights='imagenet',
          include_top=False ,
          input_shape=[100, 100, 3]
    )
    pretrained_model.trainable = False
    # fine_tune_at = 16
    # for layer in pretrained_model.layers[:fine_tune_at]:
    #   layer.trainable =  False
    x = pretrained_model(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
    model = tf.keras.Model(inputs=[i], outputs=[x])
#     model = tf.keras.Sequential([
#     tf.keras.applications.mobilenet.preprocess_input,
#     pretrained_model,
#       tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.Dense(num_classes, activation='softmax')
#     ])
    model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    #model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])
WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5
9412608/9406464 [==============================] - 0s 0us/step
20 epochs

In [20]:
model.fit(X_train, y_train, epochs=10, batch_size=1)
Epoch 1/10
33168/33168 [==============================] - 311s 9ms/step - loss: 0.3892 - accuracy: 0.9193
Epoch 2/10
33168/33168 [==============================] - 300s 9ms/step - loss: 0.0692 - accuracy: 0.9861
Epoch 3/10
33168/33168 [==============================] - 299s 9ms/step - loss: 0.0361 - accuracy: 0.9921
Epoch 4/10
33168/33168 [==============================] - 299s 9ms/step - loss: 0.0224 - accuracy: 0.9952
Epoch 5/10
33168/33168 [==============================] - 298s 9ms/step - loss: 0.0178 - accuracy: 0.9965
Epoch 6/10
33168/33168 [==============================] - 318s 10ms/step - loss: 0.0124 - accuracy: 0.9974
Epoch 7/10
33168/33168 [==============================] - 302s 9ms/step - loss: 0.0087 - accuracy: 0.9980
Epoch 8/10
33168/33168 [==============================] - 300s 9ms/step - loss: 0.0064 - accuracy: 0.9987
Epoch 9/10
33168/33168 [==============================] - 300s 9ms/step - loss: 0.0035 - accuracy: 0.9991
Epoch 10/10
33168/33168 [==============================] - 300s 9ms/step - loss: 0.0037 - accuracy: 0.9991
Out[20]:
<tensorflow.python.keras.callbacks.History at 0x7f9e92d3ff90>
In [21]:
model.evaluate(X_val, y_val)
445/445 [==============================] - 13s 27ms/step - loss: 0.0259 - accuracy: 0.9950
Out[21]:
[0.025889987125992775, 0.9950056076049805]
In [22]:
# Loading the order of the image's name that has been provided
test_image_order = pd.read_csv("/content/content/fruits_data/Testing_set.csv")
test_image_order.head()
Out[22]:
filename
0	Image_1.jpg
1	Image_2.jpg
2	Image_3.jpg
3	Image_4.jpg
4	Image_5.jpg
In [23]:
file_paths = [[fname, '/content/content/fruits_data/test/' + fname] for fname in test_image_order['filename']]
In [24]:
# Confirm if number of images is same as number of labels given
if len(test_image_order) == len(file_paths):
    print('Number of image names i.e. ', len(test_image_order), 'matches the number of file paths i.e. ', len(file_paths))
else:
    print('Number of image names does not match the number of filepaths')
Number of image names i.e.  20308 matches the number of file paths i.e.  20308
In [25]:
test_images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])
test_images.head()
Out[25]:
filename	filepaths
0	Image_1.jpg	/content/content/fruits_data/test/Image_1.jpg
1	Image_2.jpg	/content/content/fruits_data/test/Image_2.jpg
2	Image_3.jpg	/content/content/fruits_data/test/Image_3.jpg
3	Image_4.jpg	/content/content/fruits_data/test/Image_4.jpg
4	Image_5.jpg	/content/content/fruits_data/test/Image_5.jpg
In [26]:
test_pixel_data = []     # initialize an empty numpy array
image_size = 100      # image size taken is 100 here. one can take other size too
for i in range(len(test_images)):
  
  img_array = cv2.imread(test_images['filepaths'][i], cv2.IMREAD_COLOR)   # converting the image to gray scale

  new_img_array = cv2.resize(img_array, (image_size, image_size))      # resizing the image array

  test_pixel_data.append(new_img_array)
In [27]:
test_pixel_data = np.array(test_pixel_data)
In [28]:
test_pixel_data =  test_pixel_data.reshape(-1, 100, 100, 3)
In [29]:
pred = model.predict(test_pixel_data)
In [30]:
prediction = []
for value in pred:
  prediction.append(np.argmax(value))
In [31]:
predictions = le.inverse_transform(prediction)
In [32]:
res = pd.DataFrame({'filename': test_images['filename'], 'label': predictions})  # prediction is nothing but the final predictions of your model on input features of your new unseen test data
res.to_csv("submission.csv", index = False)      # the csv file will be saved locally on the same location where this notebook is located.
In [33]:
model.save_weights("pretrained_ckpt")
